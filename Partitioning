Partitioning
1) Its used for high throughput thereby scaling the system
2) Partition is called a shard in MongoDB, Elas‐ ticsearch, and SolrCloud; 
it’s known as a region in HBase, a tablet in Bigtable, a vnode in Cassandra and Riak, and a vBucket in Couchbase.

3) Partitioning is usually combined with replication so that copies of each partition are stored on multiple nodes.

Partitioning of key-Value Data
5) A partition with disproportion‐ ately high load is called a hot spot
6) To randomly distribute the data, read will become bottleneck

Partitioning By Key Range
7) an application that stores data from a network of sensors, where the key is the timestamp of the measurement (year-month-day-hour-minute-second).
8) Problem ->  If the key is a timestamp, then the partitions correspond to ranges of time—e.g., one partition per day.
9) Solution ->  use something other than the timestamp as the first element of the key. prefix each timestamp with the sensor name. 
write load will end up more evenly spread across the partitions

Partitioning By Hash Of Key
10) Because of this risk of skew and hot spots, many distributed datastores use a hash function to determine the partition for a given key
11) For partitioning purposes, the hash function need not be cryptographically strong: 
for example, Cassandra and MongoDB use MD5, and Voldemort uses the Fowler– Noll–Vo function. 
Many programming languages have simple hash functions built in (as they are used for hash tables), but they may not be suitable for partitioning: 
for example, in Java’s Object.hashCode() and Ruby’s Object#hash, the same key may have a different hash value in different processe
12) Assign each partition a range of hashes (rather than a range of keys), and every key whose hash falls within a partition’s range will be stored in that partition
13) Drawback -> lost the ability to do efficient range queries.
14) In MongoDB, if we have enabled hash-based sharding mode, any range query has to be sent to all partitions.
Range queries on the primary key are not sup‐ ported by Riak , Couchbase , or Voldemort.
15) Cassandra achieves a compromise between the two partitioning strategies.
A table in Cassandra can be declared with a compound primary key consisting of several columns.
16) The concatenated index approach enables an elegant data model for one-to-many relationships. 
For example, on a social media site, one user may post many updates. 
If the primary key for updates is chosen to be (user_id, update_timestamp), 
then you can efficiently retrieve all updates made by a particular user within some time inter‐ val, sorted by timestamp


Skewed Workloads and Relieving Hot Spots
17) Issue -> on a social media site, a celebrity user with millions of followers may cause a storm of activity when they do something 
18) Solution -> Today, most data systems are not able to automatically compensate for such a highly skewed workload, so it’s the responsibility of the application to reduce the skew. 
For example, if one key is known to be very hot, a simple technique is to add a random number to the beginning or end of the key. 
Just a two-digit decimal random number would split the writes to the key evenly across 100 different keys, allowing those keys to be distributed to different partitions.
19) Now, the problem is any reads now have to do additional work, as they have to read the data from all 100 keys and combine it
20) Perhaps in the future, data systems will be able to automatically detect and compensate for skewed workloads; 
but for now, we need to think through the trade-offs for our own application.


Partitioning and Secondary Indexes
21) The partitioning schemes we have discussed so far rely on a key-value data model
22) The situation becomes more complicated if secondary indexes are involved
23) Many key-value stores (such as HBase and Volde‐ mort) have avoided secondary indexes because of their added implementation complexity, 
but some (such as Riak) have started adding them because they are so useful for data modeling. 
And finally, secondary indexes are required for search servers such as Solr and Elasticsearch.

Partitioning Secondary Indexes by Document
24) partition the database by the document ID (for example, IDs 0 to 499 in partition 0, IDs 500 to 999 in partition 1, etc.).
25) each partition is completely separate: each partition maintains its own secondary indexes, covering only the documents in that partition. 
It doesn’t care what data is stored in other partitions.
26) document-partitioned index is also known as a local index
27) if you want to search for red cars, you need to send the query to all partitions, and combine all the results you get back. 
This approach to querying a partitioned database is sometimes known as scatter/ gather
28) Mon‐ goDB, Riak , Cassandra, Elasticsearch, SolrCloud, and VoltDB all use document-partitioned secondary indexes. 
Most database vendors recommend that you structure your partitioning scheme so that secondary index queries can be served from a single partition, 
but that is not always possible, especially when you’re using multiple secondary indexes in a single query (such as filtering cars by color and by make at the same time).

Partitioning Secondary Indexes by Term
29) Rather than each partition having its own secondary index (a local index), we can construct a global index that covers data in all partitions.
30) However, we can’t just store that index on one node, since it would likely become a bottleneck and defeat the purpose of partitioning. 
A global index must also be partitioned, but it can be partitioned differently from the primary key index
31) We can partition the index by the term itself, or using a hash of the term. 
Partitioning by the term itself can be useful for range scans (e.g., on a numeric property, such as the asking price of the car), 
whereas partitioning on a hash of the term gives a more even distribution of load.
32) We can partition the index by the term itself, or using a hash of the term. 
Partitioning by the term itself can be useful for range scans (e.g., on a numeric property, such as the asking price of the car), 
whereas partitioning on a hash of the term gives a more even distribution of load.
33) Read fast but write slow
34) The index would always be up to date, and every document written to the database would immediately be reflected in the index. 
However, in a term- partitioned index, that would require a distributed transaction across all partitions affected by a write, which is not supported in all databases
35) Updates to global secondary indexes are often asynchronous (that is, if you read the index shortly after a write, the change you just made may not yet be reflected in the index).
For example, Amazon DynamoDB states that its global secondary indexes are updated within a fraction of a second in normal circumstances, 
but may experience longer propagation delays in cases of faults in the infrastructure

Rebalancing Partitions
36) hash mod N -> Not a good approach
37) Fixed number of partitions -> create many more partitions than there are nodes, and assign several partitions to each node. 
For example, a database run‐ ning on a cluster of 10 nodes may be split into 1,000 partitions from the outset 
so that approximately 100 partitions are assigned to each node.
38) This approach to rebalancing is used in Riak, Elasticsearch, Couchbase, and Voldemort
39) In this configuration, the number of partitions is usually fixed when the database is first set up and not changed afterward
40) The best performance is achieved when the size of partitions is “just right,” neither too big nor too small, 
which can be hard to achieve if the number of partitions is fixed but the dataset size varies.

Dynamic partitioning
41) For databases that use key range partitioning, a fixed number of partitions with fixed boundaries would be very inconvenient
42) For that reason, key range–partitioned databases such as HBase and RethinkDB create partitions dynamically
43) In the case of HBase, the transfer of partition files happens through HDFS, the underlying distributed filesystem
44) A caveat is that an empty database starts off with a single partition, 
since there is no a priori information about where to draw the partition boundaries. 
While the dataset is small until it hits the point at which the first partition is split—all writes have to be processed by a single node while the other nodes sit idle.
To miti‐ gate this issue, HBase and MongoDB allow an initial set of partitions to be configured on an empty database (this is called pre-splitting).
In the case of key-range partition‐ ing, pre-splitting requires that you already know what the key distribution is going to look like. 
Dynamic partitioning is not only suitable for key range–partitioned data, but can equally well be used with hash-partitioned data. 
MongoDB since version 2.4 supports both key-range and hash partitioning, and it splits partitions dynamically in either case.

Partitioning proportionally to nodes
45) With dynamic partitioning, the number of partitions is proportional to the size of the dataset, 
since the splitting and merging processes keep the size of each partition between some fixed minimum and maximum. 
On the other hand, with a fixed number of partitions, the size of each partition is proportional to the size of the dataset
46) A third option, used by Cassandra and Ketama, is to make the number of partitions proportional to the number of nodes
47) In this case, the size of each partition grows proportion‐ ally to the dataset size while the number of nodes remains unchanged, 
but when you increase the number of nodes, the partitions become smaller again. 
Since a larger data volume generally requires a larger number of nodes to store, this approach also keeps the size of each partition fairly stable
48) When a new node joins the cluster, it randomly chooses a fixed number of existing partitions to split, 
and then takes ownership of one half of each of those split partitions while leaving the other half of each partition in place. 
The randomization can produce unfair splits, but when averaged over a larger number of partitions (in Cassandra, 256 partitions per node by default),
the new node ends up taking a fair share of the load from the existing nodes. 
Cassandra 3.0 introduced an alternative rebalancing algorithm that avoids unfair splits

Operations: Automatic or Manual Rebalancing
49) Couchbase, Riak, and Voldemort generate a suggested partition assignment automatically, but require an administrator to commit it before it takes effect
50) Automation can be dangerous in combination with automatic failure detection. 
For example, say one node is overloaded and is temporarily slow to respond to requests. 
The other nodes conclude that the overloaded node is dead, and automatically rebalance the cluster to move load away from it. 
This puts additional load on the overloaded node, other nodes, and the network making the situation worse and potentially causing a cascading failure. 
For that reason, it can be a good thing to have a human in the loop for rebalancing. 
It’s slower than a fully automatic process, but it can help prevent operational surprises

Request Routing
51) there are different approaches to routing a request to right node
i) Allow clients to contact any node (e.g., via a round-robin load balancer). 
If that node coincidentally owns the partition to which the request applies, it can handle the request directly; 
otherwise, it forwards the request to the appropriate node, receives the reply, and passes the reply along to the client.
ii) Send all requests from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly. 
This routing tier does not itself handle any requests; it only acts as a partition-aware load balancer. 
iii) Require that clients be aware of the partitioning and the assignment of partitions to nodes. 
In this case, a client can connect directly to the appropriate node, without any intermediary
52) Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata.
Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. 
Other actors, such as the routing tier or the partitioning-aware client, can subscribe to this information in ZooKeeper. 
Whenever a partition changes ownership, or a node is added or removed, ZooKeeper notifies the routing tier so that it can keep its routing information up to date
53) HBase, SolrCloud, and Kafka also use ZooKeeper to track partition assignment. 
MongoDB has a similar architecture, but it relies on its own config server implemen‐ tation and mongos daemons as the routing tier. 
Cassandra and Riak take a different approach: they use a gossip protocol among the nodes to disseminate any changes in cluster state. 
Requests can be sent to any node, and that node forwards them to the appropriate node for the requested partition. 
This model puts more complexity in the database nodes but avoids the dependency on an external coordination service such as ZooKeeper.
54) Couchbase does not rebalance automatically, which simplifies the design. 
Normally it is configured with a routing tier called moxi, which learns about routing changes from the cluster nodes.
When using a routing tier or when sending requests to a random node, clients still need to find the IP addresses to connect to. 
These are not as fast-changing as the assignment of partitions to nodes, so it is often sufficient to use DNS for this purpose.

