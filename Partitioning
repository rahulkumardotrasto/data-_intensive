Partitioning
1) Its used for high throughput thereby scaling the system
2) Partition is called a shard in MongoDB, Elas‐ ticsearch, and SolrCloud; it’s known as a region in HBase, a tablet in Bigtable, a vnode in Cassandra and Riak, and a vBucket in Couchbase.

3) Partitioning is usually combined with replication so that copies of each partition are stored on multiple nodes.

Partitioning of key-Value Data
5) A partition with disproportion‐ ately high load is called a hot spot
6) To randomly distribute the data, read will become bottleneck

Partitioning By Key Range
7) an application that stores data from a network of sensors, where the key is the timestamp of the measurement (year-month-day-hour-minute-second).
8) Problem ->  If the key is a timestamp, then the partitions correspond to ranges of time—e.g., one partition per day.
9) Solution ->  use something other than the timestamp as the first element of the key. prefix each timestamp with the sensor name. write load will end up more evenly spread across the partitions

Partitioning By Hash Of Key
10) Because of this risk of skew and hot spots, many distributed datastores use a hash function to determine the partition for a given key
11) For partitioning purposes, the hash function need not be cryptographically strong: for example, Cassandra and MongoDB use MD5, and Voldemort uses the Fowler– Noll–Vo function. Many programming languages have simple hash functions built in (as they are used for hash tables), but they may not be suitable for partitioning: for example, in Java’s Object.hashCode() and Ruby’s Object#hash, the same key may have a different hash value in different processe
12) Assign each partition a range of hashes (rather than a range of keys), and every key whose hash falls within a partition’s range will be stored in that partition
13) Drawback -> lost the ability to do efficient range queries.
14) n MongoDB, if we have enabled hash-based sharding mode, any range query has to be sent to all partitions . Range queries on the primary key are not sup‐ ported by Riak , Couchbase , or Voldemort.
15) Cassandra achieves a compromise between the two partitioning strategies . A table in Cassandra can be declared with a compound primary key consisting of several columns.
16) The concatenated index approach enables an elegant data model for one-to-many relationships. For example, on a social media site, one user may post many updates. If the primary key for updates is chosen to be (user_id, update_timestamp), then you can efficiently retrieve all updates made by a particular user within some time inter‐ val, sorted by timestamp


Skewed Workloads and Relieving Hot Spots
17) Issue -> on a social media site, a celebrity user with millions of followers may cause a storm of activity when they do something 
18) Solution -> Today, most data systems are not able to automatically compensate for such a highly skewed workload, so it’s the responsibility of the application to reduce the skew. For example, if one key is known to be very hot, a simple technique is to add a random number to the beginning or end of the key. Just a two-digit decimal random number would split the writes to the key evenly across 100 different keys, allowing those keys to be distributed to different partitions.
19) Now, the problem is any reads now have to do additional work, as they have to read the data from all 100 keys and combine it
20) Perhaps in the future, data systems will be able to automatically detect and compensate for skewed workloads; but for now, you need to think through the trade-offs for your own application.


Partitioning and Secondary Indexes
21) The partitioning schemes we have discussed so far rely on a key-value data model
22) The situation becomes more complicated if secondary indexes are involved
23) Many key-value stores (such as HBase and Volde‐ mort) have avoided secondary indexes because of their added implementation complexity, but some (such as Riak) have started adding them because they are so useful for data modeling. And finally, secondary indexes are required for search servers such as Solr and Elasticsearch.

Partitioning Secondary Indexes by Document
24) partition the database by the document ID (for example, IDs 0 to 499 in partition 0, IDs 500 to 999 in partition 1, etc.).
25) each partition is completely separate: each partition maintains its own secondary indexes, covering only the documents in that partition. It doesn’t care what data is stored in other partitions.
26) document-partitioned index is also known as a local index
27) if you want to search for red cars, you need to send the query to all partitions, and combine all the results you get back. This approach to querying a partitioned database is sometimes known as scatter/ gather
28) Mon‐ goDB, Riak [15], Cassandra [16], Elasticsearch [17], SolrCloud [18], and VoltDB [19] all use document-partitioned secondary indexes. Most database vendors recommend that you structure your partitioning scheme so that secondary index queries can be served from a single partition, but that is not always possible, especially when you’re using multiple secondary indexes in a single query (such as filtering cars by color and by make at the same time).

Partitioning Secondary Indexes by Term
29) Rather than each partition having its own secondary index (a local index), we can construct a global index that covers data in all partitions.
30)  However, we can’t just store that index on one node, since it would likely become a bottleneck and defeat the purpose of partitioning. A global index must also be partitioned, but it can be partitioned differently from the primary key index
31) We can partition the index by the term itself, or using a hash of the term. Partitioning by the term itself can be useful for range scans (e.g., on a numeric property, such as the asking price of the car), whereas partitioning on a hash of the term gives a more even distribution of load.
32) We can partition the index by the term itself, or using a hash of the term. Partitioning by the term itself can be useful for range scans (e.g., on a numeric property, such as the asking price of the car), whereas partitioning on a hash of the term gives a more even distribution of load.
33) Read fast but write slow
34) The index would always be up to date, and every document written to the database would immediately be reflected in the index. However, in a term- partitioned index, that would require a distributed transaction across all partitions affected by a write, which is not supported in all databases
35) Updates to global secondary indexes are often asynchronous (that is, if you read the index shortly after a write, the change you just made may not yet be reflected in the index). For example, Amazon DynamoDB states that its global secondary indexes are updated within a fraction of a second in normal circumstances, but may experience longer propagation delays in cases of faults in the infrastructure
